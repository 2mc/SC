{\rtf1\mac\ansicpg10000\cocoartf824\cocoasubrtf440
{\fonttbl\f0\fswiss\fcharset77 Helvetica;\f1\fnil\fcharset77 Monaco;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red191\green0\blue0;\red0\green0\blue191;
\red0\green0\blue191;\red96\green96\blue96;\red0\green115\blue0;\red0\green115\blue0;}
\deftab560
\pard\pardeftab560\ql\qnatural

\f0\fs24 \cf2 \

\f1\fs18 \cf3 //do this first: \cf2 \
\cf4 MIDIIn\cf2 .connect; 	\cf3 // init for one port midi interface\cf2 \
\
\cf0 m= \cf5 OnlineMIDI2\cf0 ();\cf2 \
\
\
(\
m.response= \cf0 nil; \cf2 \
\cf3 //now:\cf2 \
\
m.analyse(3,1.0); \cf3 //3 seconds window, step size of 1.0 seconds\cf2 \
\
\cf3 //or easier with feedback via keyboard? \cf2 \
\cf3 //feedback via MIDI slider\cf2 \
~lastreward=0;\
~reward = 0;\
\cf3 //MIDIIn.control = \{ arg src, chan, num, val; 	if(num==93,\{~reward=val/127.0;\}); \};\cf2 \
\cf0 ~training=\cf5 false\cf0 ;\
\cf2 \
\cf4 Document\cf2 .new(\cf6 "feedback"\cf2 ).keyDownAction_(\{\cf4 arg\cf2  doc, char, always, ascii;  switch(ascii, 97, \{~reward = 1;\}, 102, \{~reward = -1;\})\})\
)	\
\
\
 \
\cf0 ~sarsa=nil;\
\
~cases = List[]; \
\cf2 \
\
\cf3 //internalstate 0 = silence?\cf2 \
\cf3 //numactions = num internalstates = 5\cf2 \
(\
\cf4 var\cf2  internalstate=0,state, reward; \
\cf4 var\cf2  numstates=5**3, numactions=5; \
\cf4 var\cf2  sarsa, s2=0; \
\cf4 var\cf2  tempoclock = \cf4 TempoClock\cf2 (1);\
var tmp; \
\
sarsa = \cf0 ~sarsa ?? \{\cf4 Sarsa\cf2 (numstates, numactions);\};\
\
~sarsa= sarsa; \
\
sarsa.s= 0;\
sarsa.a=internalstate;\
\
\cf0 ~training=\cf5 true\cf0 ;\cf2 \
\cf0 m.playinput= \cf5 true\cf0 ; \
m.inputsynthdef= \cf7 \\beep2\cf0 ; \cf2 \
\
m.response = \{\cf4 |analysis|\cf2   \
	\cf4 var\cf2  environmentstate, state;\
	\cf3 //var pitches = analysis.pitches; \cf2 \
		\
	\cf3 //if (pitches.isEmpty,\{pitches= [60,62,64,65,67,69,71,72]\}); \cf2 \
	\
	environmentstate= [log10((analysis.density.min(99))+1)*0.5,(analysis.pitchvar.min(20.0))*0.05,log2(analysis.latestonset.min(3.0)+1)*0.5]; \
	\
	environmentstate.postln;\
	\
	state=(environmentstate.collect\{\cf4 |val|\cf2  if (val>=0.999999,\{val=0.999999\}); (val*5).asInteger;\}); \cf3 //++[internalstate]; //round();\cf2 \
	\
	\cf3 //10**4 = 10000 states\cf2 \
	\cf3 //combined with actions = 10**5 is 100,000!\cf2 \
	\cf3 //100*(5**3) = 12500\cf2 \
	\
	\cf3 //5 internal states\cf2 \
	\cf3 //5**5 = 3125\cf2 \
	\cf3 //action is new internal state selected\cf2 \
	\cf3 //5**4 = 625\cf2 \
	\
	\cf3 //problems; non-determinism, meaning different thing each time, immediate glare means; change now, but reward doesn't necessary accumulate over time consistently\cf2 \
	\
	\cf3 //how can you learn if only ever right or wrong in the moment? By comparing to previous situations where certain tactics worked...\cf2 \
	\cf3 //learning transition model of external stimulus (if it can be said to have one!)\cf2 \
	\
	\cf3 //or do tile coding function approximation\cf2 \
	\
	\cf3 //reward = if(~lastreward != ~reward,\{~reward;\},\{0.0\}); \cf2 \
	\
	reward= if(~reward!=0,\{~reward;\},\{0.0\});\
	~reward=0; \cf3 //so only use reward once\cf2 \
	\
	\cf0 tmp= [s2,internalstate,reward];\
	\cf2 \
	s2= 0; \
	\
	\cf3 //work out base 5\cf2 \
	state.do\{\cf4 |val, i|\cf2  s2= 5*s2+val\}; \
	\
	internalstate=if(~training,\{sarsa.train(internalstate, s2, reward);\},\{\
	sarsa.policy(internalstate, s2); \
	\});\
	\
	\cf0 ~cases.add(tmp++[s2, internalstate]); \cf2 \
	 \
	\cf3 //~lastreward = ~reward; \cf2 \
	~q=sarsa.q;\
	\
	\cf3 //schedule next second using action = internalstate\cf2 \
	\{	\
		switch(internalstate,\
		0,\{\},\
		1,\{\cf4 var\cf2  waittime = analysis.density.reciprocal;\
		\cf3 //imitation 1\cf2 \
		analysis.pitches.do\{\cf4 arg\cf2  pitch; \cf4 Synth\cf2 (\cf8 \\beep3\cf2 ,[\cf8 \\freq\cf2 , pitch.midicps]); waittime.wait;\}; \
		\},\
		2,\{\
		\cf3 //imitation 2 THIS IS ALL PITCHES FROM LAST THREE SECONDS!\cf2 \
		analysis.iois.do\{\cf4 arg\cf2  ioi; \cf4 Synth\cf2 (\cf8 \\beep3\cf2 ,[\cf8 \\freq\cf2 , rrand(60,84).midicps]); ioi.wait;\}; \
		\},\
		3,\{\cf4 var\cf2  done=0.0, nextioi;\
		\
		while(\{done<1.0\},\{nextioi=[0.5,0.25,0.1].choose; done=done+nextioi; \cf4 Synth\cf2 (\cf8 \\beep3\cf2 ,[\cf8 \\freq\cf2 , rrand(60,84).midicps]); nextioi.wait;\});\
			\
		\},\
		4,\{\
		\cf4 var\cf2  done=0.0, nextioi;\
		\
		while(\{done<1.0\},\{nextioi=(exprand(0.1,1.1)-0.1); done=done+nextioi; \cf4 Synth\cf2 (\cf8 \\beep3\cf2 ,[\cf8 \\freq\cf2 , rrand(60,84).midicps]); nextioi.wait;\})\
		\
		\});\
		\
	\}.fork(tempoclock);\
\};\
\
\
)\
\
\
\
~training=\cf4 false\cf2 ; \
\cf0 ~training=\cf5 true\cf0 ; \
\cf2 \
\
\cf0 m.response = nil; \
\
(\
130.do\{\
~sarsa.offlinetrain(~cases);\
\}\cf2 \
)\
\
\cf0 ~cases.size/60\cf2 \
\
\cf0 ~cases[0][0]=0\cf2 \
\
Post <<~cases << nl\
\cf0 Post <<~q << nl\
Post << ~sarsa.statetouched << nl\cf2 \
\
\cf0 u= ~sarsa.statetouched.select\{|val| val>0\}\cf2 \
u.size\
\
\
\cf0 ~sarsa.statetouched.sum\cf2 \
\
\
~q.plot\
\
~sarsa.alpha= 0.5; //up training rate\
\cf0 ~sarsa.eta= 0.5; //chance of going off policy\
\cf2 \
\
\cf0 ~sarsa.statetouched.plot\
\
~sarsa.statetouched.mean\cf2 \
\
\cf0 ~q.size\cf2 \
\
m.data \cf3 //poll current data\cf2 \
\
m.status = \cf4 true\cf2 ; \cf3 //prints analysis data as it goes\cf2 \
m.status= \cf4 false\cf2 ;\
\
\cf3 //use analysis data to formulate responses\cf2 \
\
\
(\
\cf4 SynthDef\cf2 (\cf8 \\beep2\cf2 ,\{\cf4 arg\cf2  freq=440,amp=0.1, pan=0.0, dur=0.1; \
\cf4 var\cf2  source; \
\
source= \cf4 SinOsc\cf2 .ar(freq*[1,1.007],0,amp*0.5);\
\
\cf4 Out\cf2 .ar(0,\cf4 Pan2\cf2 .ar(\cf4 Mix\cf2 (source)*\cf4 Line\cf2 .kr(1,0,dur, doneAction:2),pan))\}).send(s);\
\
\
\cf4 SynthDef\cf2 (\cf8 \\beep3\cf2 ,\{\cf4 arg\cf2  freq=440,amp=0.1, pan=0.0, dur=0.1; \
\cf4 var\cf2  source; \
\
source= \cf4 LFCub\cf2 .ar(freq,0,amp*0.4);\
\
\cf4 Out\cf2 .ar(0,\cf4 Pan2\cf2 .ar(source*\cf4 Line\cf2 .kr(1,0,dur, doneAction:2),pan))\}).send(s);\
\
)\
\
}